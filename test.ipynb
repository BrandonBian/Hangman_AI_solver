{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import RNN_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = RNN_model(target_dim=26, hidden_units=16)\n",
    "    checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "\n",
    "        self.guessed_letters = []\n",
    "        full_dictionary_location = \"words.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)\n",
    "        print(len(self.full_dictionary))\n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        self.freq_by_length = self.init_df(self.full_dictionary)\n",
    "        self.n_gram = self.init_n_gram(2)\n",
    "        self.current_dictionary = []\n",
    "        self.history_condition = []\n",
    "        self.model = load_model(\"model.pth\")\n",
    "\n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            requests.get(link)\n",
    "\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def find_by_gram(self, all_gram, pre=None, suff=None):\n",
    "        selected_gram = []\n",
    "        for key, val in all_gram.items():\n",
    "            if (pre is not None) and (key[0] == pre):\n",
    "                selected_gram.append((key[1], val))\n",
    "            if (suff is not None) and (key[1] == suff):\n",
    "                selected_gram.append((key[0], val))\n",
    "\n",
    "        res = {}\n",
    "        for letter, freq in selected_gram:\n",
    "            if letter not in res:\n",
    "                res[letter] = freq\n",
    "            else:\n",
    "                res[letter] += freq\n",
    "        final_res = [(key, val) for key, val in res.items()]\n",
    "        return sorted(final_res, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def gen_n_gram(self, word, n):\n",
    "        n_gram = []\n",
    "        for i in range(n, len(word)+1):\n",
    "            if word[i-n:i] not in n_gram:\n",
    "                n_gram.append(word[i-n:i])\n",
    "        return n_gram\n",
    "\n",
    "    def init_n_gram(self, n):\n",
    "        n_gram = {-1:[]}\n",
    "        for word in self.full_dictionary:\n",
    "            single_word_gram = self.gen_n_gram(word, n)\n",
    "            if len(word) not in n_gram:\n",
    "                n_gram[len(word)] = single_word_gram\n",
    "            else:\n",
    "                n_gram[len(word)].extend(single_word_gram)\n",
    "            n_gram[-1].extend(single_word_gram)\n",
    "        res = {}\n",
    "        for key in n_gram.keys():\n",
    "            res[key] = collections.Counter(n_gram[key])\n",
    "        return res\n",
    "\n",
    "    def freq_from_df(self, df):\n",
    "        key, cnt = np.unique(df.values, return_counts=True)\n",
    "        freq = [(k, val) for k, val in zip(key, cnt)]\n",
    "        return sorted(freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def update_df(self, df, condition):\n",
    "        \"\"\"\n",
    "        :param df: dataframe\n",
    "        each column is one location of a word\n",
    "        each row is a word\n",
    "        :param condition: dictionary\n",
    "        key is letter\n",
    "        value is which index does this letter appear\n",
    "        means we only select the words which has letter <value> at index <key>\n",
    "        note that we don't select words that has letter <value> at other index\n",
    "        e.g. if condition = {1:'a'}, then \"app\" is selected while \"aha\" not\n",
    "        :return:\n",
    "        df: updated dataframe\n",
    "        \"\"\"\n",
    "        if len(condition) == 0:\n",
    "            return df\n",
    "\n",
    "        for letter, idx in condition.items():\n",
    "            # find rows satisfy\n",
    "            # 1. corresponding column == val\n",
    "            # 2. all the other column != val\n",
    "            query = \"\"\n",
    "            for i in range(df.shape[1]):\n",
    "                col = df.columns.values[i]\n",
    "                if i in idx:\n",
    "                    query += \"{} == '{}' and \".format(col, letter)\n",
    "                else:\n",
    "                    query += \"{} != '{}' and \".format(col, letter)\n",
    "            query = query[:-5]\n",
    "            new_df = df.query(query)\n",
    "            df = new_df.copy()\n",
    "            del new_df\n",
    "        return df\n",
    "\n",
    "    def init_df(self, dictionary):\n",
    "        \"\"\"\n",
    "        use words list to generate dictionary frequency\n",
    "        each key is word length\n",
    "        each value is a dataframe with column is location of each length\n",
    "        \"\"\"\n",
    "        group_by_length = collections.defaultdict(list)\n",
    "        for word in dictionary:\n",
    "            group_by_length[len(word)].append(word)\n",
    "\n",
    "        res = {}\n",
    "        for key in group_by_length.keys():\n",
    "            word_list = group_by_length[key]\n",
    "            tmp = pd.DataFrame([list(word) for word in word_list])\n",
    "            tmp.columns = [chr(i + 97) for i in range(tmp.shape[1])]\n",
    "            res[key] = tmp\n",
    "        return res\n",
    "\n",
    "    def gen_condition(self, word):\n",
    "        tmp = {i: word[i] for i in range(len(word)) if word[i] != \"_\"}\n",
    "        condition = {}\n",
    "        for key, val in tmp.items():\n",
    "            if val not in condition:\n",
    "                condition[val] = [key]\n",
    "            else:\n",
    "                condition[val].append(key)\n",
    "        return condition\n",
    "\n",
    "    def encode_obscure_words(self, word):\n",
    "        word_idx = [ord(i) - 97 if i != \"_\" else 26 for i in word]\n",
    "        obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n",
    "        for i, j in enumerate(word_idx):\n",
    "            obscured_word[i, j] = 1\n",
    "        return obscured_word\n",
    "\n",
    "    def guess(self, word):  # word input example: \"_ p p _ e \"\n",
    "        word = [char for char in word if char != ' ']\n",
    "\n",
    "        # divided word group by word length\n",
    "        all_words = self.freq_by_length[len(word)]\n",
    "        all_gram = self.n_gram[-1]\n",
    "        # all_gram = self.n_gram[len(word)]\n",
    "\n",
    "        # first guess by letter frequency in each word group\n",
    "        new_condition = self.gen_condition(word)\n",
    "\n",
    "        if len(self.history_condition) != 0 and new_condition != self.history_condition[-1]:\n",
    "            self.history_condition.append(new_condition)\n",
    "\n",
    "        all_words = self.update_df(all_words, new_condition)\n",
    "        freq = self.freq_from_df(all_words)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i][0] not in self.guessed_letters:\n",
    "                return freq[i][0]\n",
    "\n",
    "        # if we run out of letters, use 2-gram to predict\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \"_\":  # this is where we should apply 2-gram\n",
    "                if (i == 0) or (word[i-1] == \"_\"):\n",
    "                    guess = self.find_by_gram(all_gram, pre=None, suff=word[i+1])\n",
    "                elif (i == len(word) - 1) or (word[i+1] == \"_\"):\n",
    "                    guess = self.find_by_gram(all_gram, pre=word[i-1], suff=None)\n",
    "                else:\n",
    "                    guess = self.find_by_gram(all_gram, pre=word[i-1], suff=word[i+1])\n",
    "                break\n",
    "\n",
    "        for i in range(len(guess)):\n",
    "            if guess[i][0] not in self.guessed_letters:\n",
    "                return guess[i][0]\n",
    "        # if we run out of 2-gram, use LSTM model to predict\n",
    "        # the benefit of LSTM model is to add more uncertainty to the prediction\n",
    "        guessed_multi_hot = np.zeros(26, dtype=np.float32)\n",
    "        for letter in self.guessed_letters:\n",
    "            idx = ord(letter) - 97\n",
    "            guessed_multi_hot[idx] = 1.0\n",
    "\n",
    "        obscure_words = self.encode_obscure_words(word)\n",
    "        obscure_words = np.asarray(obscure_words)\n",
    "        guessed_multi_hot = np.asarray(guessed_multi_hot)\n",
    "        obscure_words = torch.from_numpy(obscure_words)\n",
    "        guessed_multi_hot = torch.from_numpy(guessed_multi_hot)\n",
    "        out = self.model(obscure_words, guessed_multi_hot)\n",
    "        guess = torch.argmax(out, dim=2).item()\n",
    "        guess = chr(guess + 97)\n",
    "        print(\"Guess,\", guess)\n",
    "        return guess\n",
    "\n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location, \"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "\n",
    "    def get_current_word(self):\n",
    "        \"\"\"\n",
    "        combine target word and guessed letters to generate obscured word\n",
    "        \"\"\"\n",
    "        word_seen = [letter if letter in self.guessed_letters else \"_\" for letter in self.target_word]\n",
    "        return word_seen\n",
    "\n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "\n",
    "        print(self.current_dictionary)\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            \n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "\n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "    \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        # Add `access_token` to post_args or args if it has not already been\n",
    "        # included.\n",
    "        if self.access_token:\n",
    "            # If post_args exists, we assume that args either does not exists\n",
    "            # or it does not need `access_token`.\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "\n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HangmanAPI(access_token=\"3cf8a6d4f591b65b0049d30103c3ee\", timeout=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.start_game(practice=1,verbose=True)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "print(total_practice_successes)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
